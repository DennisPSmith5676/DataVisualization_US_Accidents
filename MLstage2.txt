Segment 2:
✓Description of preliminary data preprocessing 
✓ Description of preliminary feature engineering 
and preliminary feature selection, including their 
decision-making process 
✓ Description of how data was split into training 
and testing sets
✓ Explanation of model choice
=================================================================================================

✓Description of preliminary data preprocessing 
==================================================
 - Droped the non-beneficial columns'ID','Start_Time','End_Time','Zipcode' for machine learning process.
    ID: since they don't carry any information for the severity
    Start_Time,End_Time :because it was decomposed by the time features added before (day, month, weekday,Year)
    Zipcode :because we just focus on the City,County,state where the accident happened 
 - Dropped the Null values
 - Now this dataset contains 39 columns.
 ![dtypes](./images/mock_Dtypes.PNG)

✓ Description of preliminary feature engineering 
==================================================
and preliminary feature selection, including their 
decision-making process 
  - Street column has 17079 unique values, so used binning to catogerised the values of the Street column,
    If value count of Street less than 20 then catogerize as "Other".

   ![Bining Street](./images/biningStreet.PNG)

  - Used Label Encoding  to converte the catogerical columns  into a numeric form so as to convert them into the machine-readable form. 
    Machine learning algorithms can then decide in a better way how those labels must be operated.

  ![Label Encoded](./images/StreetLabelEncode.PNG)

 - Check features variance
   checked the variance for each feature in order to remove features with a very low variance beacuse they can't help to discriminate instances.

   ![describe](./images/Xdescribe.PNG)

    - Even though Precipitation and Pressure have a small variance, there is no need to drop them since they usually have small increments.

      ![describe](./images/DescribePercipitation.PNG)

✓ Description of how data was split into training and testing sets
===================================================================

  - train_test_split is a function in Sklearn model selection for splitting data arrays into two subsets: 
for training data and for testing data. With this function, we don't need to divide the dataset manually. 
By default, Sklearn train_test_split will make random partitions for the two subsets.
    
     ![describe](./images/splittoTrainTest.PNG)

✓ Explanation of model choice
=============================

  - Unbalanced data set
  ======================

   
    The severity attribute as we can see from the  plot is highly unbalanced, 
    the number of accident with the severity 1 is very small instead the number of accident with severity 2 is much higher.

    ![describe](./images/unbalanced.PNG)

    Train the Logistic Regression model  and calculated the accuracy score before resampling the data
  
    ![Before smapling Accuracy Score](./images/BefSampling.PNG)
    
    So, in order to balance the data set  used sampling techniques.

    Random Oversampling
    ===================
    In random oversampling, instances of the minority class are randomly selected and added to the training set until the majority and minority classes are balanced.
    
    In this section, compared two oversampling algorithms to determine which algorithm results in the best performance.
    And oversampled the data using the naive random oversampling algorithm and the SMOTE algorithm.

    Naive random oversampling algorithm
    ====================================
 
    ![Naive random oversampling Histogram](./images/Naive random oversampling-histogrem.PNG)

    Balanced Accuracy Score:

    ![Naive random oversampling Balance Acuracy Score](./images/Naive random oversampling-BAS.PNG)
    

    
    Undersampling
    ==============
    Undersampling is another technique to address class imbalance. 
    Undersampling takes the opposite approach of oversampling. Instead of increasing the number of the minority class, the size of the majority class is decreased.
    
    In this section, tested an undersampling algorithms to determine which algorithm results in the best performance compared to the oversampling algorithms above.
    undersampled the data using the Cluster Centroids algorithm.
    

   Combination of Over and Under Sampling
   ======================================
   In this section, tested a combination over- and under-sampling algorithm to determine if the algorithm results in the best performance compared to the other sampling algorithms above. Resampled the data using the SMOTEENN algorithm 

   Balanced Random Forest Classifier
   Random forest classifiers are a type of ensemble learning model that combines multiple smaller models into a more robust and accurate model. 
   Random forest models use a number of weak learner algorithms (decision trees) and combine their output to make a final classification (or regression) decision. Structurally speaking, random forest models are very similar to their neural network counterparts. 
   Random forest models have been a staple in machine learning algorithms for many years due to their robustness and scalability. Both output and feature selection of random forest models are easy to interpret, and they can easily handle outliers and nonlinear data.
  
   
 after comparing all over sampled ,under sampled and Random Forest Classifier algorithems accuracy scores we desided to proceed with Navia Over Sampling Technique and Random Forest Classifier for our final US accident datset.
    